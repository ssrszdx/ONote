## 什么是robots.txt

robots.txt是网站管理者写给爬虫的一封信，里面描述了网站管理者不希望爬虫做的事，比如：

- 不要访问某个文件、文件夹
- 禁止某些爬虫的访问
- 限制爬虫访问网站的频率

一个自觉且善意的爬虫，应该在抓取网页之前，先阅读robots.txt，了解并执行网站管理者制定的爬虫规则。

## 如何查看robot.txt

在浏览器的网址搜索栏中，输入网站的根域名，然后再输入/robot.txt。比如，必应的robots.txt网址为

> [https://cn.bing.com/robots.txt](https://link.zhihu.com/?target=https%3A//cn.bing.com/robots.txt)

## robots.txt的内容

User-agent: 爬虫的名称

Disallow: 不允许爬虫访问的地址

Allow: 允许爬虫访问的地址

若User-agent是*，则表示对象是所有爬虫。

Disallow和Allow后面跟的是地址，地址的描述格式符合正则表达式(regex)的规则。因此可以在python中使用正则表达式来筛选出可以访问的地址。

下面是来自[http://cn.bing.com](https://link.zhihu.com/?target=http%3A//cn.bing.com)的一段robots.txt：

```text
User-agent: msnbot-media 
Disallow: /
Allow: /th?

User-agent: Twitterbot
Disallow: 

User-agent: *
Disallow: /account/
Disallow: /amp/
```


[Common Crawl - Open Repository of Web Crawl Data](https://commoncrawl.org/)

[Scrapy | A Fast and Powerful Scraping and Web Crawling Framework](https://scrapy.org/)


